{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HariniMohan97/Assignment7/blob/master/Assignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7pjd_8KxXgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwaVmDpq9YPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda, SeparableConv2D, GlobalAveragePooling2D, Add\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 10\n",
        "num_filter = 20\n",
        "\n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "#cuts input (space) into 2 blocks and places on top of another (depth)\n",
        "#helps concatenating two layers with diff dimensions\n",
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=2) \n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imezzZF30Hzt",
        "colab_type": "text"
      },
      "source": [
        "Aim: To build the network shown below:-\n",
        "\n",
        "![alt text](https://2.bp.blogspot.com/-HQo2Kx39Q6A/WsqZQn5pJHI/AAAAAAAAVhY/FBkgOeS06vQNvZN2KI2hhGZmfS1cNPg8wCLcBGAs/s1600/enasdiscoverednetwork.png)\n",
        "\n",
        "##-->Functional api menthod used. \n",
        "##-->Space to Depth introduced here which helps modify dimensions of our output channel that inturn helps to concatenate with other output channels from other layers.\n",
        "\n",
        "Application:- \n",
        "\n",
        "Everytime we want to concatenate an output from a layer that comes before a max pool layer(outputA), along with an output from a layer that comes after the maxpool layer(outputB), we need to use space to depth. This operation is performed on top of outputA to match its dimensions with outputB since the dimensions of outputB is < that out outputA\n",
        "\n",
        "##-->Padding = 'same' also helps in maintaining the dimensions\n",
        "\n",
        "##-->Adding outputs of two layers helps integrate receptive fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTyJ3JRfBSO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "#32 (same) > 64 (same) > 128 (same) > 256 (same) > 512 (same) > MP\n",
        "\n",
        "\n",
        "Layer1 = SeparableConv2D(8, (5,5),padding = 'same', strides = (1,1), depth_multiplier = 1)(input) #5\n",
        "Layer1=BatchNormalization()(Layer1)\n",
        "Layer1=Activation('relu')(Layer1)\n",
        "Layer1=Dropout(0.25)(Layer1)\n",
        "\n",
        "Layer2 = Conv2D(8, (5,5), strides=(1,1), padding='same', use_bias=False)(Layer1) #9\n",
        "Layer2=BatchNormalization()(Layer2)\n",
        "Layer2=Activation('relu')(Layer2)\n",
        "Layer2=Dropout(0.25)(Layer2)\n",
        "\n",
        "Layer3 = Conv2D(8, (5,5), strides=(1,1), padding='same', use_bias=False)(Layer2)#13\n",
        "Layer3=BatchNormalization()(Layer3)\n",
        "Layer3=Activation('relu')(Layer3)\n",
        "Layer3=Dropout(0.25)(Layer3)\n",
        "\n",
        "LayerAdd1 = concatenate([Layer3,Layer1])\n",
        "\n",
        "Layer4 = SeparableConv2D(8, (5,5),padding = 'same', strides = (1,1), activation='relu', depth_multiplier = 1)(LayerAdd1)\n",
        "\n",
        "LayerAdd2 = concatenate([Layer1,Layer4])\n",
        "\n",
        "Layer5 = MaxPooling2D(pool_size=(2, 2))(LayerAdd2)\n",
        "\n",
        "Layer6 = SeparableConv2D(10, (3,3),padding = 'same', strides = (1,1), depth_multiplier = 1)(Layer5)\n",
        "Layer6=BatchNormalization()(Layer6)\n",
        "Layer6=Activation('relu')(Layer6)\n",
        "Layer6=Dropout(0.25)(Layer6)\n",
        "\n",
        "Layer4 = Lambda(space_to_depth_x2)(Layer4)\n",
        "Layer3 = Lambda(space_to_depth_x2)(Layer3)\n",
        "Layer1 = Lambda(space_to_depth_x2)(Layer1)\n",
        "Layer2 = Lambda(space_to_depth_x2)(Layer2)\n",
        "\n",
        "LayerAdd3 = concatenate([Layer4,Layer6])\n",
        "\n",
        "Layer7 = Conv2D(10, (5,5), strides=(1,1), padding='same', use_bias=False)(LayerAdd3)\n",
        "Layer7=BatchNormalization()(Layer7)\n",
        "Layer7=Activation('relu')(Layer7)\n",
        "Layer7=Dropout(0.25)(Layer7)\n",
        "\n",
        "LayerAdd4 = concatenate([Layer3,Layer7,Layer6,Layer4])\n",
        "\n",
        "Layer8 = SeparableConv2D(10, (3,3),padding = 'same', strides = (1,1), depth_multiplier = 1)(LayerAdd4)\n",
        "Layer8=BatchNormalization()(Layer8)\n",
        "Layer8=Activation('relu')(Layer8)\n",
        "Layer8=Dropout(0.25)(Layer8)\n",
        "\n",
        "LayerAdd5 = concatenate([Layer8,Layer7,Layer3,Layer6,Layer1,Layer4])\n",
        "\n",
        "Layer9 = SeparableConv2D(10, (5,5),padding = 'same', strides = (1,1), activation='relu', depth_multiplier = 1)(LayerAdd5)\n",
        "\n",
        "LayerAdd6 = concatenate([Layer8,Layer9,Layer6,Layer1,Layer4])\n",
        "\n",
        "Layer10 = MaxPooling2D(pool_size=(2, 2))(LayerAdd6)\n",
        "\n",
        "Layer7 = Lambda(space_to_depth_x2)(Layer7)\n",
        "LayerAdd7 = concatenate([Layer7,Layer10])\n",
        "\n",
        "Layer11 = Conv2D(16, (5,5), strides=(1,1), padding='same', use_bias=False)(LayerAdd7)\n",
        "Layer11=BatchNormalization()(Layer11)\n",
        "Layer11=Activation('relu')(Layer11)\n",
        "Layer11=Dropout(0.25)(Layer11)\n",
        "\n",
        "\n",
        "Layer1 = Lambda(space_to_depth_x2)(Layer1)\n",
        "Layer8 = Lambda(space_to_depth_x2)(Layer8)\n",
        "Layer2 = Lambda(space_to_depth_x2)(Layer2)\n",
        "LayerAdd8 = concatenate([Layer11,Layer1,Layer8,Layer2])\n",
        "\n",
        "Layer12 = SeparableConv2D(16, (5,5),padding = 'same', strides = (1,1), depth_multiplier = 1)(LayerAdd8)\n",
        "Layer12=BatchNormalization()(Layer12)\n",
        "Layer12=Activation('relu')(Layer12)\n",
        "Layer12=Dropout(0.25)(Layer12)\n",
        "\n",
        "Layer3 = Lambda(space_to_depth_x2)(Layer3)\n",
        "Layer6 = Lambda(space_to_depth_x2)(Layer6)\n",
        "LayerAdd9 = concatenate([Layer11,Layer12,Layer3,Layer2,Layer6])\n",
        "\n",
        "Layer13 = Conv2D(16, (3,3), strides=(1,1), padding='same', use_bias=False)(LayerAdd9)\n",
        "Layer13=BatchNormalization()(Layer13)\n",
        "Layer13=Activation('relu')(Layer13)\n",
        "Layer13=Dropout(0.25)(Layer13)\n",
        "\n",
        "Layer4 = Lambda(space_to_depth_x2)(Layer4)\n",
        "LayerAdd10 = concatenate([Layer13,Layer8,Layer3,Layer12,Layer1,Layer4,Layer6])\n",
        "\n",
        "Layer14 = SeparableConv2D(16, (5,5),padding = 'same', strides = (1,1), activation='relu', depth_multiplier = 1)(LayerAdd10)\n",
        "\n",
        "LayerAdd11 = concatenate([Layer12,Layer14,Layer8])\n",
        "\n",
        "Layer15 = Activation('softmax') (LayerAdd11)\n",
        "\n",
        "Layer16 = Conv2D(10, (1,1), strides=(1,1), padding='same', use_bias=False)(Layer15) #1x1 before GAP. to get 10 output channels finally\n",
        "\n",
        "Layer17 = GlobalAveragePooling2D()(Layer16) #GAP on the 10 obtained channels  #acts just like FCL(fully connected layer) \n",
        "\n",
        "output = (Layer17)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PmwAZVwEM7n",
        "colab_type": "code",
        "outputId": "647ccd5a-6ab0-4946-b959-53550e8c35fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_32 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_173 (Separable (None, 32, 32, 8)    107         input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 32, 32, 8)    32          separable_conv2d_173[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 32, 32, 8)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 8)    0           activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 32, 8)    1600        dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 32, 32, 8)    32          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 32, 32, 8)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 8)    0           activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 32, 8)    1600        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 32, 32, 8)    32          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 32, 32, 8)    0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 8)    0           activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_255 (Concatenate)   (None, 32, 32, 16)   0           dropout_13[0][0]                 \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_174 (Separable (None, 32, 32, 8)    536         concatenate_255[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_256 (Concatenate)   (None, 32, 32, 16)   0           dropout_11[0][0]                 \n",
            "                                                                 separable_conv2d_174[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling2D) (None, 16, 16, 16)   0           concatenate_256[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_175 (Separable (None, 16, 16, 10)   314         max_pooling2d_53[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 16, 16, 10)   40          separable_conv2d_175[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 16, 16, 10)   0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_234 (Lambda)             (None, 16, 16, 32)   0           separable_conv2d_174[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 10)   0           activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_257 (Concatenate)   (None, 16, 16, 42)   0           lambda_234[0][0]                 \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 16, 16, 10)   10500       concatenate_257[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 16, 16, 10)   40          conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 16, 16, 10)   0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 10)   0           activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_235 (Lambda)             (None, 16, 16, 32)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_258 (Concatenate)   (None, 16, 16, 84)   0           lambda_235[0][0]                 \n",
            "                                                                 dropout_15[0][0]                 \n",
            "                                                                 dropout_14[0][0]                 \n",
            "                                                                 lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_176 (Separable (None, 16, 16, 10)   1606        concatenate_258[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 16, 16, 10)   40          separable_conv2d_176[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 16, 16, 10)   0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 10)   0           activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_236 (Lambda)             (None, 16, 16, 32)   0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_259 (Concatenate)   (None, 16, 16, 126)  0           dropout_16[0][0]                 \n",
            "                                                                 dropout_15[0][0]                 \n",
            "                                                                 lambda_235[0][0]                 \n",
            "                                                                 dropout_14[0][0]                 \n",
            "                                                                 lambda_236[0][0]                 \n",
            "                                                                 lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_177 (Separable (None, 16, 16, 10)   4420        concatenate_259[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_260 (Concatenate)   (None, 16, 16, 94)   0           dropout_16[0][0]                 \n",
            "                                                                 separable_conv2d_177[0][0]       \n",
            "                                                                 dropout_14[0][0]                 \n",
            "                                                                 lambda_236[0][0]                 \n",
            "                                                                 lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_238 (Lambda)             (None, 8, 8, 40)     0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling2D) (None, 8, 8, 94)     0           concatenate_260[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_261 (Concatenate)   (None, 8, 8, 134)    0           lambda_238[0][0]                 \n",
            "                                                                 max_pooling2d_54[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 8, 8, 16)     53600       concatenate_261[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 8, 8, 16)     64          conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 8, 8, 16)     0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_237 (Lambda)             (None, 16, 16, 32)   0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 8, 8, 16)     0           activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_239 (Lambda)             (None, 8, 8, 128)    0           lambda_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_240 (Lambda)             (None, 8, 8, 40)     0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_241 (Lambda)             (None, 8, 8, 128)    0           lambda_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_262 (Concatenate)   (None, 8, 8, 312)    0           dropout_17[0][0]                 \n",
            "                                                                 lambda_239[0][0]                 \n",
            "                                                                 lambda_240[0][0]                 \n",
            "                                                                 lambda_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_178 (Separable (None, 8, 8, 16)     12808       concatenate_262[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 8, 8, 16)     64          separable_conv2d_178[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 16)     0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 8, 8, 16)     0           activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_242 (Lambda)             (None, 8, 8, 128)    0           lambda_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_243 (Lambda)             (None, 8, 8, 40)     0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_263 (Concatenate)   (None, 8, 8, 328)    0           dropout_17[0][0]                 \n",
            "                                                                 dropout_18[0][0]                 \n",
            "                                                                 lambda_242[0][0]                 \n",
            "                                                                 lambda_241[0][0]                 \n",
            "                                                                 lambda_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 8, 8, 16)     47232       concatenate_263[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 8, 8, 16)     64          conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 16)     0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 8, 8, 16)     0           activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_244 (Lambda)             (None, 8, 8, 128)    0           lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_264 (Concatenate)   (None, 8, 8, 496)    0           dropout_19[0][0]                 \n",
            "                                                                 lambda_240[0][0]                 \n",
            "                                                                 lambda_242[0][0]                 \n",
            "                                                                 dropout_18[0][0]                 \n",
            "                                                                 lambda_239[0][0]                 \n",
            "                                                                 lambda_244[0][0]                 \n",
            "                                                                 lambda_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_179 (Separable (None, 8, 8, 16)     20352       concatenate_264[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_265 (Concatenate)   (None, 8, 8, 72)     0           dropout_18[0][0]                 \n",
            "                                                                 separable_conv2d_179[0][0]       \n",
            "                                                                 lambda_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 72)     0           concatenate_265[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 8, 8, 10)     720         activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_13 (Gl (None, 10)           0           conv2d_99[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 155,803\n",
            "Trainable params: 155,599\n",
            "Non-trainable params: 204\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfulgB9BETjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIqpUJk-fW0A",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy, loss stuck in one place. Still unsure why it happens.\n",
        "## And the model takes forever to train! :'(\n",
        "## What could be done to improve the acuracy and reduce the time taken to train?\n",
        "## Is it because the network is too complex with too many params?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZKUyuZEEXGb",
        "colab_type": "code",
        "outputId": "efa60b03-32a3-48bd-9641-1472550c1ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=10,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test)) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 1027s 21ms/step - loss: 8.0343 - acc: 0.1038 - val_loss: 8.0000 - val_acc: 0.1151\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 996s 20ms/step - loss: 8.2456 - acc: 0.1003 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 1018s 20ms/step - loss: 9.6709 - acc: 0.1000 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 973s 19ms/step - loss: 9.6709 - acc: 0.1000 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 959s 19ms/step - loss: 9.6709 - acc: 0.1000 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 973s 19ms/step - loss: 9.6709 - acc: 0.1000 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 966s 19ms/step - loss: 9.6709 - acc: 0.1000 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 979s 20ms/step - loss: 9.6709 - acc: 0.1000 - val_loss: 9.6709 - val_acc: 0.1000\n",
            "Epoch 9/100\n",
            "41840/50000 [========================>.....] - ETA: 2:28 - loss: 9.6566 - acc: 0.1011"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cTL9_AeEgD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"Yolo_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNeuFiRElan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"Yolo_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")\n",
        "from google.colab import files\n",
        "\n",
        "files.download('Yolo_Basic_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTwYi-TPJwSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}